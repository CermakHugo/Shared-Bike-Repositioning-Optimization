{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b481c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_540103/4183859034.py:49: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV traités : 97\n",
      "Lignes lues : 47961198\n",
      "Erreurs : 10\n",
      "Quelques erreurs rencontrées :\n",
      " - /home/timeworid/Documents/TSI Project/data_output/divvydata/Divvy_Stations_2014-Q3Q4.csv: pas de colonnes de date valides\n",
      " - /home/timeworid/Documents/TSI Project/data_output/divvydata/Divvy_Stations_2016_Q3.csv: pas de colonnes de date valides\n",
      " - /home/timeworid/Documents/TSI Project/data_output/divvydata/Divvy_Stations_2016_Q4.csv: pas de colonnes de date valides\n",
      " - /home/timeworid/Documents/TSI Project/data_output/divvydata/Divvy_Stations_2017_Q3Q4.csv: pas de colonnes de date valides\n",
      " - /home/timeworid/Documents/TSI Project/data_output/divvydata/Divvy_Trips_2019_Q2.csv: pas de colonnes de date valides\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Config ===\n",
    "DATA_FOLDER = \"/home/timeworid/Documents/TSI Project/data_output/divvydata\"\n",
    "OUTPUT_FILE = \"/home/timeworid/Documents/TSI Project/data_output/flux/flux_par_station_par_jour.csv\"\n",
    "DATE_FORMATS = [\"%Y-%m-%d %H:%M:%S\", \"%m/%d/%Y %H:%M\", \"%Y-%m-%d %H:%M\", \"%m/%d/%Y %H:%M:%S\"]\n",
    "\n",
    "# === Log erreurs ===\n",
    "errors = []\n",
    "loaded_rows = 0\n",
    "\n",
    "# === Colonnes possibles ===\n",
    "start_time_cols = ['started_at', 'starttime', 'start_time']\n",
    "end_time_cols = ['ended_at', 'stoptime', 'end_time']\n",
    "start_station_cols = ['start_station_name', 'from_station_name']\n",
    "end_station_cols = ['end_station_name', 'to_station_name']\n",
    "bike_type_cols = ['rideable_type']\n",
    "allowed_bike_values = ['electric_bike']\n",
    "station_id_start_cols = ['start_station_id', 'from_station_id']\n",
    "station_id_end_cols = ['end_station_id', 'to_station_id']\n",
    "\n",
    "# === Flux net par (station, date) ===\n",
    "net_flow = defaultdict(int)\n",
    "\n",
    "# === Fonctions utilitaires ===\n",
    "def parse_date(date_str):\n",
    "    for fmt in DATE_FORMATS:\n",
    "        try:\n",
    "            return datetime.strptime(str(date_str), fmt).date()\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_first_available(df, columns):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            return df[col]\n",
    "    return None\n",
    "\n",
    "# === Lecture des CSV ===\n",
    "csv_files = glob.glob(os.path.join(DATA_FOLDER, \"*.csv\"))\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        loaded_rows += len(df)\n",
    "\n",
    "        # Colonnes standardisées\n",
    "        start_time = get_first_available(df, start_time_cols)\n",
    "        end_time = get_first_available(df, end_time_cols)\n",
    "        start_station = get_first_available(df, start_station_cols)\n",
    "        end_station = get_first_available(df, end_station_cols)\n",
    "        start_station_id = get_first_available(df, station_id_start_cols)\n",
    "        end_station_id = get_first_available(df, station_id_end_cols)\n",
    "        bike_type = get_first_available(df, bike_type_cols)\n",
    "\n",
    "        # Skip si pas de dates\n",
    "        if start_time is None and end_time is None:\n",
    "            errors.append(f\"{file}: pas de colonnes de date valides\")\n",
    "            continue\n",
    "\n",
    "        # Filtrer e-bikes si info dispo\n",
    "        if bike_type is not None:\n",
    "            df = df[bike_type.isin(allowed_bike_values)]\n",
    "\n",
    "        # Ajout flux départ\n",
    "        if start_time is not None and start_station is not None:\n",
    "            for s, t in zip(start_station, start_time):\n",
    "                date = parse_date(t)\n",
    "                if pd.notna(s) and date:\n",
    "                    key = (str(s).strip(), date)\n",
    "                    net_flow[key] -= 1\n",
    "\n",
    "        # Ajout flux arrivée\n",
    "        if end_time is not None and end_station is not None:\n",
    "            for s, t in zip(end_station, end_time):\n",
    "                date = parse_date(t)\n",
    "                if pd.notna(s) and date:\n",
    "                    key = (str(s).strip(), date)\n",
    "                    net_flow[key] += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        errors.append(f\"{file}: {str(e)}\")\n",
    "\n",
    "# === DataFrame final ===\n",
    "result = pd.DataFrame([\n",
    "    {'station_name': k[0], 'date': k[1], 'net_flow': v}\n",
    "    for k, v in net_flow.items()\n",
    "])\n",
    "\n",
    "# === Export CSV ===\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "# === Infos log ===\n",
    "print(f\"CSV traités : {len(csv_files)}\")\n",
    "print(f\"Lignes lues : {loaded_rows}\")\n",
    "print(f\"Erreurs : {len(errors)}\")\n",
    "if errors:\n",
    "    print(\"Quelques erreurs rencontrées :\")\n",
    "    for err in errors[:5]:\n",
    "        print(\" -\", err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V2_code\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Config ===\n",
    "DATA_FOLDER = \"/home/timeworid/Documents/TSI Project/data_output/divvydata\"\n",
    "OUTPUT_FILE = \"/home/timeworid/Documents/TSI Project/data_output/flux/flux_par_station_par_jour_pivot.csv\"\n",
    "DATE_FORMATS = [\"%Y-%m-%d %H:%M:%S\", \"%m/%d/%Y %H:%M\", \"%Y-%m-%d %H:%M\", \"%m/%d/%Y %H:%M:%S\"]\n",
    "\n",
    "# === Log erreurs ===\n",
    "errors = []\n",
    "loaded_rows = 0\n",
    "\n",
    "# === Colonnes possibles ===\n",
    "start_time_cols = ['started_at', 'starttime', 'start_time']\n",
    "end_time_cols = ['ended_at', 'stoptime', 'end_time']\n",
    "start_station_cols = ['start_station_name', 'from_station_name']\n",
    "end_station_cols = ['end_station_name', 'to_station_name']\n",
    "bike_type_cols = ['rideable_type']\n",
    "allowed_bike_values = ['electric_bike']\n",
    "\n",
    "# === Flux net par (station, date) ===\n",
    "net_flow = defaultdict(int)\n",
    "\n",
    "# === Fonctions utilitaires ===\n",
    "def parse_date(date_str):\n",
    "    for fmt in DATE_FORMATS:\n",
    "        try:\n",
    "            return datetime.strptime(str(date_str), fmt).date()\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_first_available(df, columns):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            return df[col]\n",
    "    return None\n",
    "\n",
    "# === Lecture des CSV ===\n",
    "csv_files = glob.glob(os.path.join(DATA_FOLDER, \"*.csv\"))\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        loaded_rows += len(df)\n",
    "\n",
    "        # Colonnes standardisées\n",
    "        start_time = get_first_available(df, start_time_cols)\n",
    "        end_time = get_first_available(df, end_time_cols)\n",
    "        start_station = get_first_available(df, start_station_cols)\n",
    "        end_station = get_first_available(df, end_station_cols)\n",
    "        bike_type = get_first_available(df, bike_type_cols)\n",
    "\n",
    "        # Skip si pas de dates\n",
    "        if start_time is None and end_time is None:\n",
    "            errors.append(f\"{file}: pas de colonnes de date valides\")\n",
    "            continue\n",
    "\n",
    "        # Filtrer e-bikes si info dispo\n",
    "        if bike_type is not None:\n",
    "            df = df[bike_type.isin(allowed_bike_values)]\n",
    "\n",
    "        # Ajout flux départ\n",
    "        if start_time is not None and start_station is not None:\n",
    "            for s, t in zip(start_station, start_time):\n",
    "                date = parse_date(t)\n",
    "                if pd.notna(s) and date:\n",
    "                    key = (date, str(s).strip())\n",
    "                    net_flow[key] -= 1\n",
    "\n",
    "        # Ajout flux arrivée\n",
    "        if end_time is not None and end_station is not None:\n",
    "            for s, t in zip(end_station, end_time):\n",
    "                date = parse_date(t)\n",
    "                if pd.notna(s) and date:\n",
    "                    key = (date, str(s).strip())\n",
    "                    net_flow[key] += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        errors.append(f\"{file}: {str(e)}\")\n",
    "\n",
    "# === DataFrame brut ===\n",
    "raw_df = pd.DataFrame([\n",
    "    {'date': k[0], 'station_name': k[1], 'net_flow': v}\n",
    "    for k, v in net_flow.items()\n",
    "])\n",
    "\n",
    "# === Pivot : date en ligne / station en colonne ===\n",
    "pivot_df = raw_df.pivot(index=\"date\", columns=\"station_name\", values=\"net_flow\").fillna(0).astype(int)\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# === Export CSV ===\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "pivot_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "# === Infos log ===\n",
    "print(f\"CSV traités : {len(csv_files)}\")\n",
    "print(f\"Lignes lues : {loaded_rows}\")\n",
    "print(f\"Erreurs : {len(errors)}\")\n",
    "if errors:\n",
    "    print(\"Quelques erreurs rencontrées :\")\n",
    "    for err in errors[:5]:\n",
    "        print(\" -\", err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0a0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier exporté avec ID + mapping sauvegardé.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Charger le fichier pivoté ===\n",
    "pivot_path = \"/home/timeworid/Documents/TSI Project/data_output/flux/flux_par_station_par_jour_pivot.csv\"\n",
    "df = pd.read_csv(pivot_path)\n",
    "\n",
    "# === Récupérer les noms de stations (toutes les colonnes sauf 'date') ===\n",
    "station_names = [col for col in df.columns if col != \"date\"]\n",
    "\n",
    "# === Créer un mapping nom -> ID ===\n",
    "station_id_map = {name: idx + 1 for idx, name in enumerate(station_names)}\n",
    "\n",
    "# === Renommer les colonnes ===\n",
    "df_renamed = df.rename(columns=station_id_map)\n",
    "\n",
    "# === Exporter le fichier avec IDs ===\n",
    "output_path = \"/home/timeworid/Documents/TSI Project/data_output/flux/flux_par_station_IDs.csv\"\n",
    "df_renamed.to_csv(output_path, index=False)\n",
    "\n",
    "# === Sauvegarder aussi le mapping si besoin ===\n",
    "mapping_df = pd.DataFrame(list(station_id_map.items()), columns=[\"station_name\", \"station_id\"])\n",
    "mapping_df.to_csv(\"/home/timeworid/Documents/TSI Project/data_output/flux/station_name_to_id.csv\", index=False)\n",
    "\n",
    "print(\"✅ Fichier exporté avec ID + mapping sauvegardé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2bce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichiers exportés : top 30 et autres stations.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Charger le fichier par ID ===\n",
    "df = pd.read_csv(\"/home/timeworid/Documents/TSI Project/data_output/flux/data_clean_variation.csv\")\n",
    "\n",
    "# === Calculer le flux total absolu par station ===\n",
    "flux_totaux = df.drop(columns=\"date\").abs().sum().sort_values(ascending=False)\n",
    "\n",
    "# === Top 30 stations les plus fréquentées ===\n",
    "top_30_ids = flux_totaux.head(30).index.tolist()\n",
    "\n",
    "# === Séparer les DataFrames ===\n",
    "df_top30 = df[[\"date\"] + top_30_ids]\n",
    "df_others = df.drop(columns=top_30_ids)\n",
    "\n",
    "# === Exporter ===\n",
    "df_top30.to_csv(\"/home/timeworid/Documents/TSI Project/data_output/flux/flux_top30.csv\", index=False)\n",
    "df_others.to_csv(\"/home/timeworid/Documents/TSI Project/data_output/flux/flux_other_stations.csv\", index=False)\n",
    "\n",
    "print(\"✅ Fichiers exportés : top 30 et autres stations.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
